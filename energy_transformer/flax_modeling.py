# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/11_flax_modeling.ipynb (unless otherwise specified).

__all__ = ['MaskedImageBackbone', 'DownstreamMaskedImage', 'MaskedImageForReconstruction', 'make_mlp_classifier',
           'make_linear_classifier', 'MaskedImageForClassification', 'resize_pos_embed', 'jax_resize_pos_embed']

# Cell
import functools as ft
import jax
import jax.numpy as jnp
import torch
import numpy as np
import optax
import flax
import flax.linen as nn

from typing import *
from .flax_core import HopfieldTransformer, EnergyLayerNorm
from .datasets import Patcher, imagenet_unnormalize_image
from .tools import normal
from einops import rearrange
from dataclasses import dataclass, Field, replace
from tensorboardX import SummaryWriter
from torchvision.utils import make_grid
from fastcore.meta import delegates
from fastcore.basics import patch

# Cell
class MaskedImageBackbone(nn.Module):
    patcher: Patcher
    nmask: int
    tokdim: int = 768
    nheads: int = 12
    kspace_dim:int = 64
    hidden_ratio:float = 4.
    attn_beta_init: Optional[float]=None
    use_biases_attn:bool = False
    use_biases_chn:bool = False
    use_biases_norm:bool = True
    eps:float = 1e-05
    param_dtype: Any = jnp.float32
    alpha: float = 0.1
    depth: int = 12
    dtype: Any = jnp.float32

    def setup(self):
        self.encoder = nn.Dense(self.tokdim, param_dtype=self.param_dtype)
        self.block = HopfieldTransformer(
            tokdim=self.tokdim,
            nheads=self.nheads,
            kspace_dim=self.kspace_dim,
            hidden_ratio=self.hidden_ratio,
            attn_beta_init=self.attn_beta_init,
            use_biases_attn=self.use_biases_attn,
            use_biases_chn=self.use_biases_chn,
            param_dtype=self.param_dtype
        )
        self.norm = EnergyLayerNorm(self.tokdim, param_dtype=self.param_dtype)
        self.cls_token = self.param("cls_token", nn.initializers.normal(0.002), (self.tokdim,), self.param_dtype)
        self.mask_token = self.param("mask_token", nn.initializers.normal(0.002), (self.tokdim,), self.param_dtype)
        self.pos_embed = self.param("pos_embed", nn.initializers.normal(0.002), (1 + self.patcher.n_patches, self.tokdim), self.param_dtype)

    def encode(self, x):
        """Turn x from img patches to tokens"""
        x = rearrange(x, "... c h w -> ... (c h w)")
        x = self.encoder(x)
        return x

    def do_normalize(self, x):
        return self.norm(x)

    def corrupt_tokens(
        self,
        x: jnp.ndarray,  # The input tokens of shape ND
        mask: jnp.ndarray,  # Mask (uint8) of shape N.
    ) -> jnp.ndarray:
        """Corrupt tokens `x` according to schema provided in `mask`."""
        maskmask = jnp.nonzero(mask == 1, size=self.nmask, fill_value=0)
        x = x.at[maskmask].set(self.mask_token)
        return x

    def prep_tokens(
        self,
        x: jnp.ndarray,  # Encoded tokens of shape ND
        mask: jnp.ndarray,  # uint8 mask of shape N
    ) -> jnp.ndarray:  # Tokens of shape B(N+1)D where tokens have been appropriately masked
        """Prepare tokens for masked image modeling, adding CLS,MASK tokens and POS embeddings"""
        x = self.corrupt_tokens(x, mask)
        x = jnp.concatenate([self.cls_token[None], x])
        x = x + self.pos_embed
        return x

    def forward_features(
        self,
        x: jnp.ndarray,  # Encoded, prepped tokens of shape ND
        return_features:bool= False, # Embeddings after each iteration, unnormalized
    ) -> jnp.ndarray:
        if return_features:
            embeddings = []
            energies = []

        for i in range(self.depth):
            g = self.norm(x)
            e, update = self.block.energy_and_grad(g)
            if return_features:
                embeddings.append(x)
                energies.append(e)
            x = x - (self.alpha * update)

        if return_features:
            embeddings.append(x)
            energies.append(self.block.energy(self.norm(x)))
            return x, (energies, embeddings)
        return x

    def __call__(self, x: jnp.ndarray, mask: jnp.ndarray, return_features=False):
        """Basic functionality of the backbone: given an img and mask, return the encoded representations

        With CLS token added
        """
        x = self.encode(x) # N,D
        x = self.prep_tokens(x, mask) # N+1,D
        x = self.forward_features(x, return_features=return_features) # N,D
        return x

    def collect_energies(self, x: jnp.ndarray, mask: jnp.ndarray):
        """Basic functionality of the backbone: given an img and mask, return the encoded representations and energies

        With CLS token added
        """
        x = self.encode(x) # N,D
        x = self.prep_tokens(x, mask) # N+1,D
        x, (energies, embeddings) = self.forward_features(x, True) # N,D
        return x, (energies, embeddings)

# Cell
class DownstreamMaskedImage(nn.Module):
    backbone: MaskedImageBackbone

class MaskedImageForReconstruction(DownstreamMaskedImage):
    backbone: MaskedImageBackbone

    @delegates(MaskedImageBackbone)
    @classmethod
    def from_backbone_args(cls, patcher, nmask, **kwargs):
        backbone = MaskedImageBackbone(
            patcher=patcher,
            nmask=nmask,
            **kwargs
        )
        return cls(backbone)

    def setup(self):
        self.decoder = nn.Dense(self.backbone.patcher.patch_elements, param_dtype=self.backbone.param_dtype)

    def do_normalize(self, x):
        return self.backbone.do_normalize(x)

    def decode(self, x):
        x = self.backbone.norm(x)
        x = self.decoder(x)
        c,h,w = self.backbone.patcher.patch_shape
        return rearrange(x, "... (c h w) -> ... c h w", c=c, h=h, w=w)

    def __call__(self, x:jnp.ndarray, mask:jnp.ndarray):
        x = self.backbone(x, mask)
        x = x[1:]
        x = self.decode(x)
        return x

    def collect_energies(self, x:jnp.ndarray, mask:jnp.ndarray):
        x, (energies, embeddings) = self.backbone.collect_energies(x, mask)
        x = x[1:]
        x = self.decode(x)
        return x, (energies, embeddings)

# Cell
def make_mlp_classifier(n_classes:int):
    """Default factory for MLP classification head"""
    MLPClassifier = nn.Sequential([
        nn.LayerNorm(),
        nn.Dense(n_classes),
        nn.relu,
        nn.Dense(n_classes)
    ])
    return MLPClassifier

def make_linear_classifier(n_classes:int):
    """Default factory for linear classification head"""
    LinearClassifier = nn.Sequential([
        nn.LayerNorm(),
        nn.Dense(n_classes)])
    return LinearClassifier

class MaskedImageForClassification(DownstreamMaskedImage):
    backbone: MaskedImageBackbone
    n_classes: int
    head_type:str = "mlp" # "linear" or "mlp"

    @delegates(MaskedImageBackbone)
    @classmethod
    def from_backbone_args(cls, patcher, n_classes, head_type="mlp", **kwargs):
        backbone = MaskedImageBackbone(
            patcher=patcher,
            nmask=0,
            **kwargs
        )
        return cls(backbone, n_classes=n_classes, head_type=head_type)

    def setup(self):
        assert self.head_type in set(["mlp", "linear"])
        if self.head_type == "mlp":
            cls_head = make_mlp_classifier(self.n_classes)
        elif self.head_type == "linear":
            cls_head = make_linear_classifier(self.n_classes)
        else:
            raise ValueError(f"Head type {self.head_type} not supported")
        self.cls_head = cls_head

    def __call__(self, x:jnp.ndarray):
        """Return the logits for a fully unmasked image"""
        x = self.backbone.encode(x) # N,D
        # x = self.prep_tokens(x, mask) # N+1,D
        x = jnp.concatenate([self.backbone.cls_token[None], x])
        x = x + self.backbone.pos_embed
        x = self.backbone.forward_features(x) # N,D
        cls_tok = x[0]
        logits = self.cls_head(cls_tok)
        return logits

# Cell
import torch
import torch.nn.functional as F
import numpy as np
import math

def resize_pos_embed(posemb, posemb_new, num_tokens=1, gs_new=()):
    # Rescale the grid of position embeddings when loading from state_dict. Adapted from
    # https://github.com/google-research/vision_transformer/blob/00883dd691c63a6830751563748663526e811cee/vit_jax/checkpoint.py#L224
    ntok_new = posemb_new.shape[1]
    if num_tokens:
        posemb_tok, posemb_grid = posemb[:, :num_tokens], posemb[0, num_tokens:]
        ntok_new -= num_tokens
    else:
        posemb_tok, posemb_grid = posemb[:, :0], posemb[0]
    gs_old = int(math.sqrt(len(posemb_grid)))
    if not len(gs_new):  # backwards compatibility
        gs_new = [int(math.sqrt(ntok_new))] * 2
    assert len(gs_new) >= 2
    posemb_grid = posemb_grid.reshape(1, gs_old, gs_old, -1).permute(0, 3, 1, 2)
    posemb_grid = F.interpolate(posemb_grid, size=gs_new, mode='bicubic', align_corners=False)
    posemb_grid = posemb_grid.permute(0, 2, 3, 1).reshape(1, gs_new[0] * gs_new[1], -1)
    posemb = torch.cat([posemb_tok, posemb_grid], dim=1)
    return posemb

def jax_resize_pos_embed(old_posemb, new_posemb, num_tokens=1, gs_new=()):
    """Thin wrapper around `resize_pos_embed` from TIMM to work with modified position embeddings"""
    p1 = torch.from_numpy(np.array(old_posemb)).unsqueeze(0)
    p2 = torch.from_numpy(np.array(new_posemb)).unsqueeze(0)
    out = resize_pos_embed(p1, p2, num_tokens=num_tokens, gs_new=gs_new)
    return jnp.array(out[0])