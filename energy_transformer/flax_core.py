# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/10_flax_core.ipynb (unless otherwise specified).

__all__ = ['ifnone', 'MultiheadAttention', 'CHNReLU', 'CHNSoftmax', 'EnergyLayerNorm', 'RegularizedEnergyLayerNorm',
           'HopfieldTransformer']

# Cell
import numpy as np
import jax
import jax.numpy as jnp
import flax
import flax.linen as nn
from typing import *
import matplotlib.pyplot as plt
from .tools import *
import functools as ft
from dataclasses import dataclass
import warnings

# Cell
def ifnone(a,b):
    if a is None: return b
    return a

## Multihead Attention
class MultiheadAttention(nn.Module):
    """The energy of attention for a single head"""
    tokdim: int=768
    nheads: int = 12
    kspace_dim: int = 64
    use_bias: bool = False
    param_dtype: Any = jnp.float32
    beta_init: Optional[float] = None

    def setup(self):
        self.Wk = self.param("Wk", nn.initializers.normal(0.002), (self.nheads, self.kspace_dim, self.tokdim), self.param_dtype)
        self.Wq = self.param("Wq", nn.initializers.normal(0.002), (self.nheads, self.kspace_dim, self.tokdim), self.param_dtype)
        self.betas = jnp.ones(self.nheads, dtype=self.param_dtype) * ifnone(self.beta_init, 1/jnp.sqrt(self.kspace_dim))

        if self.use_bias:
            bq = self.param("bq", self.bias_init, (self.nheads, self.kspace_dim), self.param_dtype)
            bk = self.param("bk", self.bias_init, (self.nheads, self.kspace_dim), self.param_dtype)

    def energy(self, g:jnp.ndarray):
        """Return the energy of the block"""
        K = jnp.einsum("kd,hzd->khz", g, self.Wk) # kseq,heads,kspace
        Q = jnp.einsum("qd,hzd->qhz", g, self.Wq) # qseq,heads,kspace
        if self.use_bias:
            K = K + self.bk
            Q = Q + self.bq

        A1 = jnp.einsum("h,qhz,khz->hqk", self.betas, Q, K) # heads, qseq, kseq
        A2 = jax.nn.logsumexp(A1, -1) # heads, qseq
        A3 = A2.sum(-1) # heads
        A4 = (-1/self.betas * A3).sum()
        return A4

    def energy_and_grad(self, g:jnp.ndarray):
        return jax.value_and_grad(self.energy)(g)

    def manual_grad(self, g:jnp.ndarray) -> jnp.ndarray:
        K = jnp.einsum("hzd,kd->khz", self.Wk, g)
        Q = jnp.einsum("hzd,qd->qhz", self.Wq, g)
        F1 = jnp.einsum("hzd,khz->khd", self.Wq, K)
        F2 = jnp.einsum("hzd,qhz->qhd", self.Wk, Q)

        A1 = jnp.einsum("h,khz,qhz->hqk", self.betas, K, Q)
        A2 = jax.nn.softmax(A1, -1) # hqk

        T1 = -jnp.einsum("khd,hqk->qd", F1, A2)
        T2 = -jnp.einsum("qhd,hqk->kd", F2, A2)
        return T1 + T2


# Cell
class CHNReLU(nn.Module):
    tokdim: int
    hidden_ratio: float = 4.
    param_dtype:Any = jnp.float32
    use_bias: bool=False

    def setup(self):
        hid_dim = int(self.hidden_ratio * self.tokdim)
        self.kernel = self.param("kernel", nn.initializers.normal(0.02), (self.tokdim, hid_dim), self.param_dtype)
        if self.use_bias:
            self.bias = self.param("bias", nn.initializers.zeros, (hid_dim,), self.param_dtype)

    def energy(self, g:jnp.ndarray):
        h = g @ self.kernel
        if self.use_bias:
            h += self.bias
        A = jax.nn.relu(h)
        return -0.5*(A**2).sum()

    def energy_and_grad(self, g:jnp.ndarray):
        return jax.value_and_grad(self.energy)(g)

    def manual_grad(self, g: jnp.ndarray):
        """Only used for checking the automatic gradient calculation"""
        h = g @ self.kernel
        if self.use_bias:
            h += self.bias
        A1 = jax.nn.relu(h) # hid
        A2 = -A1 @ self.kernel.T # D
        return A2

# Cell
class CHNSoftmax(nn.Module):
    tokdim: int
    hidden_ratio: float = 4.
    param_dtype: Any = jnp.float32
    use_bias: bool=False
    beta_init: float = 0.01

    def setup(self):
        hid_dim = int(self.hidden_ratio * self.tokdim)
        self.kernel = self.param("kernel", nn.initializers.normal(0.02), (self.tokdim, hid_dim), self.param_dtype)
        self.beta = self.param(
            "beta",
            lambda key, shape, dtype: nn.initializers.ones(key, shape, dtype)*self.beta_init,
            (), self.param_dtype)
        if self.use_bias:
            self.bias = self.param("bias", nn.initializers.zeros, (hid_dim,), self.param_dtype)

    def energy(self, g:jnp.ndarray):
        h = self.beta * g @ self.kernel
        if self.use_bias:
            h = h + self.bias
        A = jax.nn.logsumexp(h, axis=-1) # hid
        return -1/self.beta * A.sum()

    def energy_and_grad(self, g:jnp.ndarray):
        return jax.value_and_grad(self.energy)(g)

    def manual_grad(self, g:jnp.ndarray):
        """Only used for checking the automatic gradient calculation"""
        h = self.beta * g @ self.kernel
        if self.use_bias:
            h = h + self.bias
        A1 = jax.nn.softmax(h, axis=-1) # hid
        A2 = -A1 @ self.kernel.T # D
        return A2

# Cell
class EnergyLayerNorm(nn.Module):
    """Do layer norm on the last dimension of input

    While an energy could be defined for this, it is easier to just define the forward operation (activation function) since the
    energy calculation is not needed for the dynamics of the network
    """
    dim: int
    param_dtype: Any = jnp.float32
    use_bias:bool = True # Whether to use a bias in the layer normalization step or not
    eps: float = 1e-05 # Prevent division by 0

    @nn.compact
    def __call__(self, x: jnp.ndarray):
        gamma = self.param("gamma", nn.initializers.ones, (), self.param_dtype)
        if self.use_bias:
            bias = self.param ("bias", nn.initializers.zeros, (self.dim,), self.param_dtype)
        xmeaned = x - x.mean(-1, keepdims=True)
        v = gamma * (xmeaned) / jnp.sqrt((xmeaned**2).mean(-1, keepdims=True)+self.eps)
        if self.use_bias:
            return v + bias
        return v

# Cell
class RegularizedEnergyLayerNorm(nn.Module):
    """Do layer norm on the last dimension of input

    While an energy could be defined for this, it is easier to just define the forward operation (activation function) since the
    energy calculation is not needed for the dynamics of the network
    """
    dim: int
    param_dtype: Any = jnp.float32
    use_bias:bool = True # Whether to use a bias in the layer normalization step or not
    eps: float = 1e-05 # Prevent division by 0
    lmda: float = 1e-03 # Regularization

    @nn.compact
    def __call__(self, x: jnp.ndarray):
        gamma = self.param("gamma", nn.initializers.ones, (), self.param_dtype)
        if self.use_bias:
            bias = self.param ("bias", nn.initializers.zeros, (self.dim,), self.param_dtype)
        xmeaned = x - x.mean(-1, keepdims=True)
        v = gamma * (xmeaned) / jnp.sqrt((xmeaned**2).mean(-1, keepdims=True)+self.eps)
        v = v + self.lmda * x
        if self.use_bias:
            return v + bias
        return v

# Cell
class HopfieldTransformer(nn.Module):
    """Full energy transformer"""
    tokdim: int = 768
    nheads: int = 12
    kspace_dim:int = 64
    hidden_ratio:float = 4.
    attn_beta_init: Optional[float] = None
    use_biases_attn:bool = False
    use_biases_chn:bool = False
    param_dtype:Any = jnp.float32

    def setup(self):
        self.attn = MultiheadAttention(
            tokdim=self.tokdim,
            nheads=self.nheads,
            kspace_dim=self.kspace_dim,
            use_bias=self.use_biases_attn,
            beta_init=self.attn_beta_init,
            param_dtype=self.param_dtype
        )
        self.chn = CHNReLU(tokdim=self.tokdim, hidden_ratio=self.hidden_ratio, param_dtype=self.param_dtype)

    def energy(self, g:jnp.ndarray):
        attn_energy = self.attn.energy(g)
        chn_energy = self.chn.energy(g)
        return attn_energy + chn_energy

    def manual_grad(self, g:jnp.ndarray):
        return self.attn.manual_grad(g) + self.chn.manual_grad(g)

    def energy_and_grad(self, g:jnp.ndarray):
        return jax.value_and_grad(self.energy)(g)